<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Thomas Wang on SBMARUF</title>
    <link>https://sbmaruf.github.io/authors/thomas-wang/</link>
    <description>Recent content in Thomas Wang on SBMARUF</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 03 Nov 2022 00:00:00 +0300</lastBuildDate>
    
	<atom:link href="https://sbmaruf.github.io/authors/thomas-wang/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Crosslingual Generalization through Multitask Finetuning</title>
      <link>https://sbmaruf.github.io/publication/xmtf/</link>
      <pubDate>Thu, 03 Nov 2022 00:00:00 +0300</pubDate>
      
      <guid>https://sbmaruf.github.io/publication/xmtf/</guid>
      <description>Shows multitask multilingual generalization in language model.</description>
    </item>
    
    <item>
      <title>What Language Model to Train if You Have One Million GPU Hours?</title>
      <link>https://sbmaruf.github.io/publication/llm/</link>
      <pubDate>Tue, 01 Mar 2022 00:00:00 +0300</pubDate>
      
      <guid>https://sbmaruf.github.io/publication/llm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multitask Prompted Training Enables Zero-Shot Task Generalization</title>
      <link>https://sbmaruf.github.io/publication/t0pp/</link>
      <pubDate>Fri, 15 Oct 2021 00:00:00 +0300</pubDate>
      
      <guid>https://sbmaruf.github.io/publication/t0pp/</guid>
      <description>T0 shows zero-shot task generalization on English natural language prompts, outperforming GPT-3 on many tasks, while being 16x smaller!</description>
    </item>
    
  </channel>
</rss>