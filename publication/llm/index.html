<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.1.0">
  <meta name="generator" content="Hugo 0.55.6" />

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="M Saiful Bari">

  
  
  
    
  
  <meta name="description" content="The crystallization of modeling methods around the Transformer architecture has been a boon for practitioners. Simple, well-motivated architectural variations that transfer across tasks and scale, increasing the impact of modeling research. However, with the emergence of state-of-the-art 100B&#43; parameters models, large language models are increasingly expensive to accurately design and train. Notably, it can be difficult to evaluate how modeling decisions may impact emergent capabilities, given that these capabilities arise mainly from sheer scale.Targeting a multilingual language model in the 100B&#43; parameters scale, our goal is to identify an architecture and training setup that makes the best use of our 1,000,000 A100-GPU-hours budget. Specifically, we perform an ablation study comparing different modeling practices and their impact on zero-shot generalization. We perform all our experiments on 1.3B models, providing a compromise between compute costs and the likelihood that our conclusions will hold for the target 100B&#43; model. In addition, we study the impact of various popular pretraining corpora on zero-shot generalization. We also study the performance of a multilingual model and how it compares to the English-only one. Finally, we consider the scaling behaviour of Transformers to chose the target model size, shape, and training setup.">

  
  <link rel="alternate" hreflang="en-us" href="https://sbmaruf.github.io/publication/llm/">

  


  

  

  

  

  

  

  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha256-eSi1q2PG6J7g7ib17yAaWMcrr5GrtohYChqibrV7PBE=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:400,700|Merriweather|Roboto+Mono">
  

  <link rel="stylesheet" href="/styles.css">
  

  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-120208312-1', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  
  <link rel="alternate" href="https://sbmaruf.github.io/index.xml" type="application/rss+xml" title="SBMARUF">
  <link rel="feed" href="https://sbmaruf.github.io/index.xml" type="application/rss+xml" title="SBMARUF">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://sbmaruf.github.io/publication/llm/">

  
  
  
  
    
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@sbmaruf">
  <meta property="twitter:creator" content="@sbmaruf">
  
  <meta property="og:site_name" content="SBMARUF">
  <meta property="og:url" content="https://sbmaruf.github.io/publication/llm/">
  <meta property="og:title" content="What Language Model to Train if You Have One Million GPU Hours? | SBMARUF">
  <meta property="og:description" content="The crystallization of modeling methods around the Transformer architecture has been a boon for practitioners. Simple, well-motivated architectural variations that transfer across tasks and scale, increasing the impact of modeling research. However, with the emergence of state-of-the-art 100B&#43; parameters models, large language models are increasingly expensive to accurately design and train. Notably, it can be difficult to evaluate how modeling decisions may impact emergent capabilities, given that these capabilities arise mainly from sheer scale.Targeting a multilingual language model in the 100B&#43; parameters scale, our goal is to identify an architecture and training setup that makes the best use of our 1,000,000 A100-GPU-hours budget. Specifically, we perform an ablation study comparing different modeling practices and their impact on zero-shot generalization. We perform all our experiments on 1.3B models, providing a compromise between compute costs and the likelihood that our conclusions will hold for the target 100B&#43; model. In addition, we study the impact of various popular pretraining corpora on zero-shot generalization. We also study the performance of a multilingual model and how it compares to the English-only one. Finally, we consider the scaling behaviour of Transformers to chose the target model size, shape, and training setup."><meta property="og:image" content="https://sbmaruf.github.io/publication/llm/featured.png">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2022-03-01T00:00:00-05:00">
  
  <meta property="article:modified_time" content="2022-03-01T00:00:00-05:00">
  

  

  

  <title>What Language Model to Train if You Have One Million GPU Hours? | SBMARUF</title>

</head>
<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >
  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" role="textbox" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">SBMARUF</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#experience">
            
            <span>Experience</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#talks">
            
            <span>Talk</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#publications">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/M_Saiful_Bari_CV.pdf">
            
            <span>CV</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#news">
            
            <span>News</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/post/awards">
            
            <span>Awards</span>
            
          </a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>

<div class="pub" itemscope itemtype="http://schema.org/CreativeWork">

  













<div class="article-header d-xl-none">
  <div class="featured-image" style="background-image: url('/publication/llm/featured_hu62caba80ea58cda8cca21b58e77b5806_57441_800x0_resize_lanczos_2.png');"></div>
  <span class="article-header-caption">Image credit: <a href="https://openreview.net/pdf?id=rI7BL3fHIZq" target="_blank"><strong>Paper</strong></a></span>
</div>


<div class="container-fluid split-header d-none d-xl-block">
  <div class="row">
    <div class="col-6">
      <div class="split-header-content">
        <h1 itemprop="name">What Language Model to Train if You Have One Million GPU Hours?</h1>

        

        



<meta content="2022-03-01 00:00:00 -0500 EST" itemprop="datePublished">
<meta content="2022-03-01 00:00:00 -0500 EST" itemprop="dateModified">

<div class="article-metadata">

  
  
  
  
  <div>
    



  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="">Teven Le Scao</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="">Thomas Wang</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="">Daniel Hesslow</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="">Lucile Saulnier</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="">Stas Bekman</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="">M Saiful Bari</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="">Stella Biderman</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="">Hady Elsahar</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="">Jason Phang</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="">Ofir Press</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="">Colin Raffel</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="">Victor Sanh</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="">Sheng Shen</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="">Lintang Sutawika</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="">Jaesung Tae</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="">Zheng Xin Yong</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="">Julien Launay</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="">Iz Beltagy</a></span></span>
  



  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    <time>March 1, 2022</time>
  </span>
  

  

  

  
  

  

  

</div>


        







  






  



<div class="btn-links mb-3">
  
  







  




<button type="button" class="btn btn-outline-primary my-1 mr-1 js-cite-modal"
        data-filename="/publication/llm/cite.bib">
  Cite
</button>














  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary my-1 mr-1" href="https://openreview.net/pdf?id=rI7BL3fHIZq" target="_blank" rel="noopener">
    
    Paper
  </a>


</div>



        
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=What%20Language%20Model%20to%20Train%20if%20You%20Have%20One%20Million%20GPU%20Hours%3f&amp;url=https%3a%2f%2fsbmaruf.github.io%2fpublication%2fllm%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fsbmaruf.github.io%2fpublication%2fllm%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-facebook-f"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsbmaruf.github.io%2fpublication%2fllm%2f&amp;title=What%20Language%20Model%20to%20Train%20if%20You%20Have%20One%20Million%20GPU%20Hours%3f"
         target="_blank" rel="noopener">
        <i class="fab fa-linkedin-in"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=https%3a%2f%2fsbmaruf.github.io%2fpublication%2fllm%2f&amp;title=What%20Language%20Model%20to%20Train%20if%20You%20Have%20One%20Million%20GPU%20Hours%3f"
         target="_blank" rel="noopener">
        <i class="fab fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=What%20Language%20Model%20to%20Train%20if%20You%20Have%20One%20Million%20GPU%20Hours%3f&amp;body=https%3a%2f%2fsbmaruf.github.io%2fpublication%2fllm%2f">
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


      </div>
    </div>
    <div class="col-6">
      <div class="split-header-image">
        <img src="/publication/llm/featured_hu62caba80ea58cda8cca21b58e77b5806_57441_680x500_fill_q90_lanczos_center_2.png" itemprop="image" alt="">
        <span class="article-header-caption">Image credit: <a href="https://openreview.net/pdf?id=rI7BL3fHIZq" target="_blank"><strong>Paper</strong></a></span>
      </div>
    </div>
  </div>
</div>

<div class="article-container d-xl-none">
  <h1 itemprop="name">What Language Model to Train if You Have One Million GPU Hours?</h1>

  

  



<meta content="2022-03-01 00:00:00 -0500 EST" itemprop="datePublished">
<meta content="2022-03-01 00:00:00 -0500 EST" itemprop="dateModified">

<div class="article-metadata">

  
  
  
  
  <div>
    



  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="">Teven Le Scao</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="">Thomas Wang</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="">Daniel Hesslow</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="">Lucile Saulnier</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="">Stas Bekman</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="">M Saiful Bari</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="">Stella Biderman</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="">Hady Elsahar</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="">Jason Phang</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="">Ofir Press</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="">Colin Raffel</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="">Victor Sanh</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="">Sheng Shen</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="">Lintang Sutawika</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="">Jaesung Tae</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="">Zheng Xin Yong</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="">Julien Launay</a></span></span>, 
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">
        

      
      
      <a href="">Iz Beltagy</a></span></span>
  



  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    <time>March 1, 2022</time>
  </span>
  

  

  

  
  

  

  
    
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=What%20Language%20Model%20to%20Train%20if%20You%20Have%20One%20Million%20GPU%20Hours%3f&amp;url=https%3a%2f%2fsbmaruf.github.io%2fpublication%2fllm%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fsbmaruf.github.io%2fpublication%2fllm%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-facebook-f"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsbmaruf.github.io%2fpublication%2fllm%2f&amp;title=What%20Language%20Model%20to%20Train%20if%20You%20Have%20One%20Million%20GPU%20Hours%3f"
         target="_blank" rel="noopener">
        <i class="fab fa-linkedin-in"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=https%3a%2f%2fsbmaruf.github.io%2fpublication%2fllm%2f&amp;title=What%20Language%20Model%20to%20Train%20if%20You%20Have%20One%20Million%20GPU%20Hours%3f"
         target="_blank" rel="noopener">
        <i class="fab fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=What%20Language%20Model%20to%20Train%20if%20You%20Have%20One%20Million%20GPU%20Hours%3f&amp;body=https%3a%2f%2fsbmaruf.github.io%2fpublication%2fllm%2f">
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>

  







  






  



<div class="btn-links mb-3">
  
  







  




<button type="button" class="btn btn-outline-primary my-1 mr-1 js-cite-modal"
        data-filename="/publication/llm/cite.bib">
  Cite
</button>














  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary my-1 mr-1" href="https://openreview.net/pdf?id=rI7BL3fHIZq" target="_blank" rel="noopener">
    
    Paper
  </a>


</div>


</div>



  <div class="article-container">

    
    <h3>Abstract</h3>
    <p class="pub-abstract" itemprop="text">The crystallization of modeling methods around the Transformer architecture has been a boon for practitioners. Simple, well-motivated architectural variations that transfer across tasks and scale, increasing the impact of modeling research. However, with the emergence of state-of-the-art 100B+ parameters models, large language models are increasingly expensive to accurately design and train. Notably, it can be difficult to evaluate how modeling decisions may impact emergent capabilities, given that these capabilities arise mainly from sheer scale.Targeting a multilingual language model in the 100B+ parameters scale, our goal is to identify an architecture and training setup that makes the best use of our 1,000,000 A100-GPU-hours budget. Specifically, we perform an ablation study comparing different modeling practices and their impact on zero-shot generalization. We perform all our experiments on 1.3B models, providing a compromise between compute costs and the likelihood that our conclusions will hold for the target 100B+ model. In addition, we study the impact of various popular pretraining corpora on zero-shot generalization. We also study the performance of a multilingual model and how it compares to the English-only one. Finally, we consider the scaling behaviour of Transformers to chose the target model size, shape, and training setup.</p>
    

    
    <div class="row">
      <div class="col-md-1"></div>
      <div class="col-md-10">
        <div class="row">
          <div class="col-12 col-md-3 pub-row-heading">Type</div>
          <div class="col-12 col-md-9">
            

            
            
            <a href="/publication/#1">
              Conference paper
            </a>
            
          </div>
        </div>
      </div>
      <div class="col-md-1"></div>
    </div>
    <div class="d-md-none space-below"></div>
    

    
    <div class="row">
      <div class="col-md-1"></div>
      <div class="col-md-10">
        <div class="row">
          <div class="col-12 col-md-3 pub-row-heading">Publication</div>
          <div class="col-12 col-md-9">Challenges &amp; Perspectives in Creating Large Language Models</div>
        </div>
      </div>
      <div class="col-md-1"></div>
    </div>
    <div class="d-md-none space-below"></div>
    

    <div class="space-below"></div>

    <div class="article-style"></div>

    


<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/deep-learning/">deep-learning</a>
  
  <a class="badge badge-light" href="/tags/multi-lingual/">multi-lingual</a>
  
  <a class="badge badge-light" href="/tags/language-model/">language-model</a>
  
</div>



    






  
  
    
  
  








  </div>
</div>



<div class="container">
  <footer class="site-footer">
  

  <p class="powered-by">
    

    M Saiful Bari © 2020 | <a href="https://github.com/sbmaruf" target="_blank" rel="noopener">sbmaruf</a> |

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

</div>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha512-+NqPlbbtM1QqiK8ZAo4Yrj2c4lNQoGv8P79DPtKzj++l5jnN39rHA/xsqn8zE9l0uSoxaCdrOgFs6yjyfbBxSg==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha256-VsEqElsCHSGmnmHXGQzvoWjWwoznFSZc6hs7ARLRacQ=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.d7381f2d79e6271d4da28f474f49096c.js"></script>

  </body>
</html>

