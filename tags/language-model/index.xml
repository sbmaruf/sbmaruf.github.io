<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>language-model on SBMARUF</title>
    <link>https://sbmaruf.github.io/tags/language-model/</link>
    <description>Recent content in language-model on SBMARUF</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 01 May 2021 00:00:00 +0800</lastBuildDate>
    
	<atom:link href="https://sbmaruf.github.io/tags/language-model/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>AugVic: Exploiting BiText Vicinity for Low-Resource NMT</title>
      <link>https://sbmaruf.github.io/publication/augvic/</link>
      <pubDate>Sat, 01 May 2021 00:00:00 +0800</pubDate>
      
      <guid>https://sbmaruf.github.io/publication/augvic/</guid>
      <description>We propose AugVic, a data augmentation framework for sequence to sequence model (i.e. NMT) using Language Model.</description>
    </item>
    
    <item>
      <title>UXLA: A Robust Unsupervised Data Augmentation Framework for Zero-Resouce Cross-Lingual NLP</title>
      <link>https://sbmaruf.github.io/publication/uxla/</link>
      <pubDate>Thu, 30 Apr 2020 00:00:00 +0800</pubDate>
      
      <guid>https://sbmaruf.github.io/publication/uxla/</guid>
      <description>We propose UXLA, a novel data augmentation framework for self-supervised learning in zero-resource transfer learning scenarios.</description>
    </item>
    
    <item>
      <title>mBART: pretraining seq2seq architecture</title>
      <link>https://sbmaruf.github.io/talk/mbart/</link>
      <pubDate>Sat, 25 Apr 2020 11:30:00 +0000</pubDate>
      
      <guid>https://sbmaruf.github.io/talk/mbart/</guid>
      <description>This talk summarizes the paper &lt;a href=&#34;https://arxiv.org/abs/2001.08210&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;mBART&lt;/code&gt;&lt;/a&gt; and some pretraining concepts.</description>
    </item>
    
    <item>
      <title>Transforme-XL</title>
      <link>https://sbmaruf.github.io/talk/transformer-xl/</link>
      <pubDate>Fri, 05 Jul 2019 15:30:00 +0000</pubDate>
      
      <guid>https://sbmaruf.github.io/talk/transformer-xl/</guid>
      <description>This talk summarizes the paper &lt;a href=&#34;https://arxiv.org/abs/1901.02860&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context&lt;/code&gt;&lt;/a&gt;. It assumes that audience are already familier with &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;Attention Is All You Need&lt;/code&gt;&lt;/a&gt; paper and also discuss some high level concepts of it.</description>
    </item>
    
  </channel>
</rss>