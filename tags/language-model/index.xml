<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>language-model on SBMARUF</title>
    <link>https://sbmaruf.github.io/tags/language-model/</link>
    <description>Recent content in language-model on SBMARUF</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 29 May 2023 00:00:00 +0800</lastBuildDate>
    
	<atom:link href="https://sbmaruf.github.io/tags/language-model/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets</title>
      <link>https://sbmaruf.github.io/publication/chatgpt/</link>
      <pubDate>Mon, 29 May 2023 00:00:00 +0800</pubDate>
      
      <guid>https://sbmaruf.github.io/publication/chatgpt/</guid>
      <description>The paper comprehensively evaluates ChatGPT&amp;rsquo;s performance on various academic tasks, covering 140 tasks across diverse fields, highlighting strengths and weaknesses, and introducing a new ability to follow multi-query instructions, ultimately paving the way for practical applications of ChatGPT-like models.</description>
    </item>
    
    <item>
      <title>Pathways to semi-(un)supervised* NLP Brain</title>
      <link>https://sbmaruf.github.io/talk/pathways-llm/</link>
      <pubDate>Mon, 01 May 2023 12:30:00 +0000</pubDate>
      
      <guid>https://sbmaruf.github.io/talk/pathways-llm/</guid>
      <description>This talk discusses the evolving field of transfer learning, from LSTMs to large language models, and  shows new direction on the transferability in large language model.</description>
    </item>
    
    <item>
      <title>xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval</title>
      <link>https://sbmaruf.github.io/publication/xcodeeval/</link>
      <pubDate>Mon, 06 Mar 2023 00:00:00 +0800</pubDate>
      
      <guid>https://sbmaruf.github.io/publication/xcodeeval/</guid>
      <description>We introduce xCodeEval, the largest executable multilingual multitask benchmark to date consisting of 25M document-level coding examples from about 7.5 K unique problems covering up to 17 programming languages with execution-level parallelism.</description>
    </item>
    
    <item>
      <title>Crosslingual Generalization through Multitask Finetuning</title>
      <link>https://sbmaruf.github.io/publication/xmtf/</link>
      <pubDate>Thu, 03 Nov 2022 00:00:00 +0800</pubDate>
      
      <guid>https://sbmaruf.github.io/publication/xmtf/</guid>
      <description>Shows multitask multilingual generalization in language model.</description>
    </item>
    
    <item>
      <title>What Language Model to Train if You Have One Million GPU Hours?</title>
      <link>https://sbmaruf.github.io/publication/llm/</link>
      <pubDate>Tue, 01 Mar 2022 00:00:00 +0800</pubDate>
      
      <guid>https://sbmaruf.github.io/publication/llm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts</title>
      <link>https://sbmaruf.github.io/publication/promptsource/</link>
      <pubDate>Wed, 02 Feb 2022 00:00:00 +0800</pubDate>
      
      <guid>https://sbmaruf.github.io/publication/promptsource/</guid>
      <description>Over 2,000 prompts for roughly 170 datasets are available through PromptSource framework.</description>
    </item>
    
    <item>
      <title>Multitask Prompted Training Enables Zero-Shot Task Generalization</title>
      <link>https://sbmaruf.github.io/publication/t0pp/</link>
      <pubDate>Fri, 15 Oct 2021 00:00:00 +0800</pubDate>
      
      <guid>https://sbmaruf.github.io/publication/t0pp/</guid>
      <description>T0 shows zero-shot task generalization on English natural language prompts, outperforming GPT-3 on many tasks, while being 16x smaller!</description>
    </item>
    
    <item>
      <title>Nearest Neighbour Few-Shot Learning for Cross-lingual Classification</title>
      <link>https://sbmaruf.github.io/publication/nn/</link>
      <pubDate>Thu, 23 Sep 2021 00:00:00 +0800</pubDate>
      
      <guid>https://sbmaruf.github.io/publication/nn/</guid>
      <description>We propose a trasductive approach for few shot cross-lingual classification.</description>
    </item>
    
    <item>
      <title>AugVic: Exploiting BiText Vicinity for Low-Resource NMT</title>
      <link>https://sbmaruf.github.io/publication/augvic/</link>
      <pubDate>Sat, 01 May 2021 00:00:00 +0800</pubDate>
      
      <guid>https://sbmaruf.github.io/publication/augvic/</guid>
      <description>We propose AugVic, a data augmentation framework for sequence to sequence model (i.e. NMT) using Language Model.</description>
    </item>
    
    <item>
      <title>UXLA: A Robust Unsupervised Data Augmentation Framework for Zero-Resouce Cross-Lingual NLP</title>
      <link>https://sbmaruf.github.io/publication/uxla/</link>
      <pubDate>Thu, 30 Apr 2020 00:00:00 +0800</pubDate>
      
      <guid>https://sbmaruf.github.io/publication/uxla/</guid>
      <description>We propose UXLA, a novel data augmentation framework for self-supervised learning in zero-resource transfer learning scenarios.</description>
    </item>
    
    <item>
      <title>mBART: pretraining seq2seq architecture</title>
      <link>https://sbmaruf.github.io/talk/mbart/</link>
      <pubDate>Sat, 25 Apr 2020 11:30:00 +0000</pubDate>
      
      <guid>https://sbmaruf.github.io/talk/mbart/</guid>
      <description>This talk summarizes the paper &lt;a href=&#34;https://arxiv.org/abs/2001.08210&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;mBART&lt;/code&gt;&lt;/a&gt; and some pretraining concepts.</description>
    </item>
    
    <item>
      <title>Transforme-XL</title>
      <link>https://sbmaruf.github.io/talk/transformer-xl/</link>
      <pubDate>Fri, 05 Jul 2019 15:30:00 +0000</pubDate>
      
      <guid>https://sbmaruf.github.io/talk/transformer-xl/</guid>
      <description>This talk summarizes the paper &lt;a href=&#34;https://arxiv.org/abs/1901.02860&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context&lt;/code&gt;&lt;/a&gt;. It assumes that audience are already familier with &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;Attention Is All You Need&lt;/code&gt;&lt;/a&gt; paper and also discuss some high level concepts of it.</description>
    </item>
    
  </channel>
</rss>