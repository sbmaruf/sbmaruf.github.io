[{"authors":["admin"],"categories":null,"content":"M Saiful Bari (Maruf) is the Training Lead and one of the Core Maintainers of ALLaM, a sovereign foundational model for English and Arabic language technologies. His research focuses on understanding and advancing large language models, particularly in the areas of scaling, training dynamics, and systematic evaluation of frontier (-\u0026gt; superintelligence) models. His lab investigates these aspects through three primary areas: (1) scaling behaviors and training dynamics of large language models in terms of data and truthfulness, (2) developing robust evaluation methodologies for frontier class (-\u0026gt; superintelligence) models with scalable-oversight, and (3) exploring efficient learning paradigms through transfer learning at scale. This research combines theoretical frameworks with large-scale empirical studies, leading to both methodological innovations and practical applications.\nMaruf received his Ph.D. from Nanyang Technological University under the supervision of Prof. Shafiq Joty, where his thesis explored transfer learning for large language model adaptation, addressing the fundamental question: How can you learn so much from so little?. During his doctoral studies, he attended three internships at Amazon Web Services (2021-2023) and made substantial contributions to the BLOOM LLM development, particularly in its architecture design, pretraining, and prompt engineering efforts.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"https://sbmaruf.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"M Saiful Bari (Maruf) is the Training Lead and one of the Core Maintainers of ALLaM, a sovereign foundational model for English and Arabic language technologies. His research focuses on understanding and advancing large language models, particularly in the areas of scaling, training dynamics, and systematic evaluation of frontier (-\u0026gt; superintelligence) models. His lab investigates these aspects through three primary areas: (1) scaling behaviors and training dynamics of large language models in terms of data and truthfulness, (2) developing robust evaluation methodologies for frontier class (-\u0026gt; superintelligence) models with scalable-oversight, and (3) exploring efficient learning paradigms through transfer learning at scale.","tags":null,"title":"M Saiful Bari","type":"author"},{"authors":null,"categories":null,"content":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.main]] menu links to it in the config.toml.\n","date":1536440400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536440400,"objectID":"c3224f3a64174f08aaf31e1f1d16ffd3","permalink":"https://sbmaruf.github.io/tutorial/","publishdate":"2018-09-09T00:00:00+03:00","relpermalink":"/tutorial/","section":"tutorial","summary":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.","tags":null,"title":"Overview","type":"docs"},{"authors":[],"categories":null,"content":" Click on the **Slides** button above to view the built-in slides feature.   -- With the release of ChatGPT, there has been a paradigm shift in Generative AI and its applications. Despite this groundbreaking perception, the underlying technology has remained relatively consistent over the past five years. What sets ChatGPT apart is the refined integration of traditional methods with advanced data engineering to optimize human preferences, resulting in a sophisticated tool that has revolutionized the industry. This talk will dive into the technical details and the evolution of Large Language Models (LLMs) over the past 4-5 years. At the end, we will explore the potential for a knowledge explosion facilitated by generative models in the coming years. During the talk, Iâ€™ll emphasize the tips and tricks for training truly large LLMs, explain FLOPS per politics and show you the timeline of ChatGPT\u0026rsquo;s development and my experiences as a GPU-poor dancing around the sea of flops. I hope you come scaling your humor like actual Large LLMs.\n","date":1723971600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1723971600,"objectID":"bbbc2906870f5d26b0ba983ea1ed76ec","permalink":"https://sbmaruf.github.io/talk/chatgptmoment/","publishdate":"2024-08-18T00:00:00+03:00","relpermalink":"/talk/chatgptmoment/","section":"talk","summary":"The talk summarizes how ChatGPT's release marked a turning point in Generative AI, driven by refined integration of traditional methods with advanced data engineering for optimizing human preferences.","tags":["Deep-Learning"],"title":"The ChatGPT moment: The past, current and future (potential) of LLMs","type":"talk"},{"authors":["Md Tahmid Rahman Laskar","M Saiful Bari","Mizanur Rahman","Md Amran Hossen Bhuiyan","Shafiq Joty","Jimmy Xiangji Huang"],"categories":null,"content":"","date":1685307600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685307600,"objectID":"616c151d9b6e05b05c67c020775bd4ca","permalink":"https://sbmaruf.github.io/publication/chatgpt/","publishdate":"2023-05-29T00:00:00+03:00","relpermalink":"/publication/chatgpt/","section":"publication","summary":"The paper comprehensively evaluates ChatGPT's performance on various academic tasks, covering 140 tasks across diverse fields, highlighting strengths and weaknesses, and introducing a new ability to follow multi-query instructions, ultimately paving the way for practical applications of ChatGPT-like models.","tags":["deep-learning","evaluation","language-model"],"title":"A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets","type":"publication"},{"authors":["M Saiful Bari"],"categories":null,"content":"This talk discusses the evolving field of transfer learning, from LSTMs to large language models, and shows new direction on the transferability in large language model.\n Click on the **Slides** button above to view the built-in slides feature.   -- ","date":1682944200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682944200,"objectID":"51095917f8be4d41c4524733075e60e2","permalink":"https://sbmaruf.github.io/talk/pathways-llm/","publishdate":"2023-05-01T00:00:00+03:00","relpermalink":"/talk/pathways-llm/","section":"talk","summary":"This talk discusses the evolving field of transfer learning, from LSTMs to large language models, and  shows new direction on the transferability in large language model.","tags":["deep-learning","language-model","semi-supervised-learning"],"title":"Pathways to semi-(un)supervised* NLP Brain","type":"talk"},{"authors":["Mohammad Abdullah Matin Khan","M Saiful Bari","Xuan Long Do","Weishi Wang","Md Rizwan Parvez","Shafiq Joty"],"categories":null,"content":"","date":1678050000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1678050000,"objectID":"b13d00725fb037e45fea2dc6096404fb","permalink":"https://sbmaruf.github.io/publication/xcodeeval/","publishdate":"2023-03-06T00:00:00+03:00","relpermalink":"/publication/xcodeeval/","section":"publication","summary":"We introduce xCodeEval, the largest executable multilingual multitask benchmark to date consisting of 25M document-level coding examples from about 7.5 K unique problems covering up to 17 programming languages with execution-level parallelism.","tags":["deep-learning","multi-lingual","language-model","programming-language"],"title":"xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval","type":"publication"},{"authors":["Niklas Muennighoff","Thomas Wang","Lintang Sutawika","Adam Roberts","Stella Biderman","Teven Le Scao","M Saiful Bari","Sheng Shen","Zheng-Xin Yong","Hailey Schoelkopf","Xiangru Tang","Dragomir Radev","Alham Fikri Aji","Khalid Almubarak","Samuel Albanie","Zaid Alyafeai","Albert Webson","Edward Raff","Colin Raffel"],"categories":null,"content":"","date":1667422800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667422800,"objectID":"339a934dcc6d13bae8d5770c459544c8","permalink":"https://sbmaruf.github.io/publication/xmtf/","publishdate":"2022-11-03T00:00:00+03:00","relpermalink":"/publication/xmtf/","section":"publication","summary":"Shows multitask multilingual generalization in language model.","tags":["deep-learning","language-model"],"title":"Crosslingual Generalization through Multitask Finetuning","type":"publication"},{"authors":["Teven Le Scao","Thomas Wang","Daniel Hesslow","Lucile Saulnier","Stas Bekman","M Saiful Bari","Stella Biderman","Hady Elsahar","Jason Phang","Ofir Press","Colin Raffel","Victor Sanh","Sheng Shen","Lintang Sutawika","Jaesung Tae","Zheng Xin Yong","Julien Launay","Iz Beltagy"],"categories":null,"content":"","date":1646082000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646082000,"objectID":"f27f88c5b012aabb65eff53d095a7b65","permalink":"https://sbmaruf.github.io/publication/llm/","publishdate":"2022-03-01T00:00:00+03:00","relpermalink":"/publication/llm/","section":"publication","summary":"The crystallization of modeling methods around the Transformer architecture has been a boon for practitioners. Simple, well-motivated architectural variations that transfer across tasks and scale, increasing the impact of modeling research. However, with the emergence of state-of-the-art 100B+ parameters models, large language models are increasingly expensive to accurately design and train. Notably, it can be difficult to evaluate how modeling decisions may impact emergent capabilities, given that these capabilities arise mainly from sheer scale.Targeting a multilingual language model in the 100B+ parameters scale, our goal is to identify an architecture and training setup that makes the best use of our 1,000,000 A100-GPU-hours budget. Specifically, we perform an ablation study comparing different modeling practices and their impact on zero-shot generalization. We perform all our experiments on 1.3B models, providing a compromise between compute costs and the likelihood that our conclusions will hold for the target 100B+ model. In addition, we study the impact of various popular pretraining corpora on zero-shot generalization. We also study the performance of a multilingual model and how it compares to the English-only one. Finally, we consider the scaling behaviour of Transformers to chose the target model size, shape, and training setup.","tags":["deep-learning","multi-lingual","language-model"],"title":"What Language Model to Train if You Have One Million GPU Hours?","type":"publication"},{"authors":["M Saiful Bari"],"categories":null,"content":" Click on the **Slides** button above to view the built-in slides feature.   -- ","date":1644251400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644251400,"objectID":"c8c1ddc0c9553303c8b3824af1632706","permalink":"https://sbmaruf.github.io/talk/t0++/","publishdate":"2022-02-07T00:00:00+03:00","relpermalink":"/talk/t0++/","section":"talk","summary":"A talk on T0++ paper.","tags":["deep-learning","architecture"],"title":"Multitask Prompted Training Enables Zero-Shot Task Generalization","type":"talk"},{"authors":["Stephen H. Bach","Victor Sanh","Zheng-Xin Yong","Albert Webson","Colin Raffel","Nihal V. Nayak","Abheesht Sharma","Taewoon Kim","M Saiful Bari","Thibault Fevry","Zaid Alyafeai","Manan Dey","Andrea Santilli","Zhiqing Sun","Srulik Ben-David","Canwen Xu","Gunjan Chhablani","Han Wang","Jason Alan Fries","Maged S. Al-shaibani","Shanya Sharma","Urmish Thakker","Khalid Almubarak","Xiangru Tang","Xiangru Tang","Mike Tian-Jian Jiang","Alexander M. Rush"],"categories":null,"content":"","date":1643749200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643749200,"objectID":"035f81d807b1440f9b479b826b522c85","permalink":"https://sbmaruf.github.io/publication/promptsource/","publishdate":"2022-02-02T00:00:00+03:00","relpermalink":"/publication/promptsource/","section":"publication","summary":"Over 2,000 prompts for roughly 170 datasets are available through PromptSource framework.","tags":["deep-learning","language-model"],"title":"PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts","type":"publication"},{"authors":["Victor Sanh","Albert Webson","Colin Raffel","Stephen H. Bach","Lintang Sutawika","Zaid Alyafeai","Antoine Chaffin","Arnaud Stiegler","Teven Le Scao","Arun Raja","Manan Dey","M Saiful Bari","Canwen Xu","Urmish Thakker","Shanya Sharma Sharma","Eliza Szczechla","Taewoon Kim","Gunjan Chhablani","Nihal Nayak","Debajyoti Datta","Jonathan Chang","Mike Tian-Jian Jiang","Han Wang","Matteo Manica","Sheng Shen","Zheng Xin Yong","Harshit Pandey","Rachel Bawden","Thomas Wang","Trishala Neeraj","Jos Rozen","Abheesht Sharma","Andrea Santilli","Thibault Fevry","Jason Alan Fries","Ryan Teehan","Stella Biderman","Leo Gao","Tali Bers","Thomas Wolf","Alexander M. Rush"],"categories":null,"content":"","date":1634245200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634245200,"objectID":"66c6ebc251826d342cb12beedeca2a38","permalink":"https://sbmaruf.github.io/publication/t0pp/","publishdate":"2021-10-15T00:00:00+03:00","relpermalink":"/publication/t0pp/","section":"publication","summary":"T0 shows zero-shot task generalization on English natural language prompts, outperforming GPT-3 on many tasks, while being 16x smaller!","tags":["deep-learning","language-model"],"title":"Multitask Prompted Training Enables Zero-Shot Task Generalization","type":"publication"},{"authors":["M Saiful Bari","Batool Haider","Saab Mansour"],"categories":null,"content":"","date":1632344400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632344400,"objectID":"7312130cf7c958697583d5337e586665","permalink":"https://sbmaruf.github.io/publication/nn/","publishdate":"2021-09-23T00:00:00+03:00","relpermalink":"/publication/nn/","section":"publication","summary":"We propose a trasductive approach for few shot cross-lingual classification.","tags":["deep-learning","cross-lingual","language-model"],"title":"Nearest Neighbour Few-Shot Learning for Cross-lingual Classification","type":"publication"},{"authors":[],"categories":null,"content":" Click on the **Slides** button above to view the built-in slides feature.   -- ","date":1632306600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632306600,"objectID":"c108e162044669c22b0ff7997a53a5b7","permalink":"https://sbmaruf.github.io/talk/flan/","publishdate":"2021-09-22T00:00:00+03:00","relpermalink":"/talk/flan/","section":"talk","summary":"This talk summarizes the paper [`Finetuned Language Models Are Zero-Shot Learners`](https://arxiv.org/abs/2109.01652).","tags":["Deep-Learning"],"title":"Finetuned Language Models Are Zero-Shot Learners","type":"talk"},{"authors":[],"categories":null,"content":"","date":1622376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622376000,"objectID":"3db9138a4303c1dc0dc94edac90b370c","permalink":"https://sbmaruf.github.io/talk/gpt-3/","publishdate":"2021-05-30T00:00:00+03:00","relpermalink":"/talk/gpt-3/","section":"talk","summary":"This talk summarizes the paper [`Language Models are Few-Shot Learners`](https://arxiv.org/abs/2005.14165).","tags":["Deep-Learning"],"title":"GPT-3: Language Models are Few-Shot Learners","type":"talk"},{"authors":["Tasnim Mohiuddin","M Saiful Bari","Shafiq Joty"],"categories":null,"content":"","date":1619816400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619816400,"objectID":"f04a2be6a5e5e85f8095671139c2e453","permalink":"https://sbmaruf.github.io/publication/augvic/","publishdate":"2021-05-01T00:00:00+03:00","relpermalink":"/publication/augvic/","section":"publication","summary":"We propose AugVic, a data augmentation framework for sequence to sequence model (i.e. NMT) using Language Model.","tags":["deep-learning","cross-lingual","language-model"],"title":"AugVic: Exploiting BiText Vicinity for Low-Resource NMT","type":"publication"},{"authors":["M Saiful Bari","Tasnim Mohiuddin","Shafiq Joty"],"categories":null,"content":"","date":1588194000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588194000,"objectID":"bf7ca0648cb00755bd72785c80ee3407","permalink":"https://sbmaruf.github.io/publication/uxla/","publishdate":"2020-04-30T00:00:00+03:00","relpermalink":"/publication/uxla/","section":"publication","summary":"We propose UXLA, a novel data augmentation framework for self-supervised learning in zero-resource transfer learning scenarios.","tags":["deep-learning","cross-lingual","language-model"],"title":"UXLA: A Robust Unsupervised Data Augmentation Framework for Zero-Resouce Cross-Lingual NLP","type":"publication"},{"authors":["Tasnim Mohiuddin","M Saiful Bari","Shafiq Joty"],"categories":null,"content":"","date":1588021200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588021200,"objectID":"c2e197584e836ad959efb3b402fe2a91","permalink":"https://sbmaruf.github.io/publication/lnmap/","publishdate":"2020-04-28T00:00:00+03:00","relpermalink":"/publication/lnmap/","section":"publication","summary":"We propose a novel semi-supervised method to learn cross-lingual word embeddings for BLI.","tags":["deep-learning","cross-lingual","word-embedding"],"title":"LNMAP: Departures from Isomorphic Assumption in Bilingual Lexicon Induction Through Non-Linear Mapping in Latent Space","type":"publication"},{"authors":[],"categories":null,"content":"This talk goes though the pretraining objectives of seq2seq architecture. It also discusses, how mBART is different from pretraining of XLM and it\u0026rsquo;s derivatives?\n Click on the **Slides** button above to view the built-in slides feature.   -- ","date":1587814200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587814200,"objectID":"052b1eef0aaacb5f2287dd25669b7169","permalink":"https://sbmaruf.github.io/talk/mbart/","publishdate":"2018-04-25T00:00:00+03:00","relpermalink":"/talk/mbart/","section":"talk","summary":"This talk summarizes the paper [`mBART`](https://arxiv.org/abs/2001.08210) and some pretraining concepts.","tags":["deep-learning","language-model","machine-translation"],"title":"mBART: pretraining seq2seq architecture","type":"talk"},{"authors":["M Saiful Bari","Shafiq Joty","Prathyusha Jwalapuram"],"categories":null,"content":"","date":1573678800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573678800,"objectID":"0a893120330887630c35b55013714c33","permalink":"https://sbmaruf.github.io/publication/cross-lingual-ner/","publishdate":"2019-11-14T00:00:00+03:00","relpermalink":"/publication/cross-lingual-ner/","section":"publication","summary":"We propose a superior model and training method for zero resource transfer of Cross-lingual Named Entity Recognition.","tags":["deep-learning","cross-lingual","word-embedding"],"title":"Zero-Resource Cross-Lingual Named Entity Recognition","type":"publication"},{"authors":[],"categories":null,"content":" Click on the **Slides** button above to view the built-in slides feature.   -- ","date":1565019000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565019000,"objectID":"4b6a7ae3c6b39ef5119f3fb6243ec96b","permalink":"https://sbmaruf.github.io/talk/semi-sup/","publishdate":"2018-03-21T00:00:00+03:00","relpermalink":"/talk/semi-sup/","section":"talk","summary":"This talk summarizes the paper [`mixup`](https://arxiv.org/abs/1710.09412), [`MixMatch`](https://arxiv.org/abs/1905.02249), [`DivideMix`](https://openreview.net/pdf?id=HJgExaVtwr), [`UDA`](https://arxiv.org/abs/1904.12848).","tags":["deep-learning","semi-sup-learning"],"title":"Semi-Supervised Training","type":"talk"},{"authors":[],"categories":null,"content":" Click on the **Slides** button above to view the built-in slides feature.   -- ","date":1562340600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562340600,"objectID":"9a63bb05156d88080313bde1cc0183af","permalink":"https://sbmaruf.github.io/talk/transformer-xl/","publishdate":"2019-07-05T00:00:00+03:00","relpermalink":"/talk/transformer-xl/","section":"talk","summary":"This talk summarizes the paper [`Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context`](https://arxiv.org/abs/1901.02860). It assumes that audience are already familier with [`Attention Is All You Need`](https://arxiv.org/abs/1706.03762) paper and also discuss some high level concepts of it.","tags":["deep-learning","architecture","language-model"],"title":"Transforme-XL","type":"talk"},{"authors":["Xiang Lin","Shafiq Joty","Prathyusha Jwalapuram","M Saiful Bari"],"categories":null,"content":"","date":1557867600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557867600,"objectID":"5d166dc4fed2fd3b8c79b2624add7f23","permalink":"https://sbmaruf.github.io/publication/pointer-net-parser/","publishdate":"2019-05-15T00:00:00+03:00","relpermalink":"/publication/pointer-net-parser/","section":"publication","summary":"We propose an efficient neural framework for sentence-level discourse analysis in accordance with Rhetorical Structure Theory (RST). Our  framework comprises a discourse segmenter to identify the elementary discourse units (EDU) in a text, and a discourse parser that constructs a discourse  tree in a top-down fashion. Both the segmenter and the parser are based on Pointer Networks and operate in linear time. Our segmenter yields an F1 score  of 95.4, and our parser achieves an F1 score of 81.7 on the aggregated labeled (relation) metric, surpassing previous approaches by a good margin and  approaching human agreement on both tasks (98.3 and 83.0 F1).","tags":["discouse-parsing"],"title":"A Unified Linear-Time Framework for Sentence-Level Discourse Parsing","type":"publication"},{"authors":["M Saiful Bari"],"categories":[],"content":" from IPython.core.display import Image Image('https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png')  print(\u0026quot;Welcome to Academic!\u0026quot;)  Welcome to Academic!  Install Python and Jupyter Install Anaconda which includes Python 3 and Jupyter notebook.\nOtherwise, for advanced users, install Jupyter notebook with pip3 install jupyter.\nCreate a new blog post as usual Run the following commands in your Terminal, substituting \u0026lt;MY_WEBSITE_FOLDER\u0026gt; and my-post with the file path to your Academic website folder and a name for your blog post (without spaces), respectively:\ncd \u0026lt;MY_WEBSITE_FOLDER\u0026gt; hugo new --kind post post/my-post cd \u0026lt;MY_WEBSITE_FOLDER\u0026gt;/content/post/my-post/  Create or upload a Jupyter notebook Run the following command to start Jupyter within your new blog post folder. Then create a new Jupyter notebook (New \u0026gt; Python Notebook) or upload a notebook.\njupyter notebook  Convert notebook to Markdown jupyter nbconvert Untitled.ipynb --to markdown --NbConvertApp.output_files_dir=. # Copy the contents of Untitled.md and append it to index.md: cat Untitled.md | tee -a index.md # Remove the temporary file: rm Untitled.md  Edit your post metadata Open index.md in your text editor and edit the title etc. in the front matter according to your preference.\nTo set a featured image, place an image named featured into your post\u0026rsquo;s folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"https://sbmaruf.github.io/post/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/jupyter/","section":"post","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"post"},{"authors":null,"categories":null,"content":" C++ Implementation of variety of Algorithms and some mush have cheetsheets for CS Students.\nCheet Sheets  Trigonometry Cheat sheet Computer Science Cheat sheet : Mainly series, algorithm discrete math and calculus based cheat sheet. ICPC Cheatsheet : Outdated, may be helpful for beginners.  Data Structure In no particular order,\n- Binary Indexed tree (BIT) - Heavy Light Decomposition (HLD) - Histrogram - LCA - RMQ - trie  Geometry In no particular order,\n- CircleSegmentTetrahedron - Closest Pair - ConvexHull - ConvexHull GrahamScan - ConvexHull MonotoneChain - Parametric Geometry routine - Line segment intersection - Ray casting algorithm (PointInPolygon) - Rotate point - Tangent of line  Graph In no particular order, - Stoer Wagner all pair Min Cut - Articulation Point - Bellman Ford - BiConnected Component - Bridge - Disjoint Set - Eular Circuit - Hungerian Algorithm - Max Weighted Bi-partite Matching - MaxFlow Dinic - Maximum Bipertite Matching - Mincost Max Flow - Minimum Expression - Dinitz - Dinitz With EdgeList - Stable marrige problem - Strongly Connected Component - Tarjans Off line LCA - manacher\nMatrix \u0026amp; Numeric In no particular order,\n- Big float (C++ library) - BigInt - FFT - Faussian Elimination - matrix Exponentiation  Number theory and Math In no particular order,\n- ExtendedEuclidMOdInverse - Hn - LinearDiphontine - Number Theory Part 1.pdf - Good colelction of Number theoric discussion. - NumberTheory Part 2.pdf - Good colelction of Number theoric discussion. - PollardRho - SegmentedSieve - ShankBabyStepGiantStep - Sieve - josepheous - ncr  Searching - Ternary Search  String - Aho Chorasik - KMP - Hashing - suffix-array.pdf - Good discussion of suffix-array - Suffix array code.  IO - Fast read C++  Collected Library  Stanford University ACM Team Notebook : Outdated, maybe helpful for mid-level/above mid-level problem solver.  Combinatorial optimization1.  Sparse max-flow (C++) Min-cost max-flow (C++) Push-relabel max-flow (C++) Min-cost matching (C++) Max bipartite matching (C++) Global min cut (C++) Graph cut inference (C++)  Geometry  Convex hull (C++) Miscellaneous geometry (C++) Java geometry (Java) 3D geometry (Java) Slow Delaunay triangulation (C++)  Numerical algorithms  Number theoretic algorithms (modular, Chinese remainder, linear Diophantine) (C++) Systems of linear equations, matrix inverse, determinant (C++) Reduced row echelon form, matrix rank (C++) Fast Fourier transform (C++) Simplex algorithm (C++)  Graph algorithms  Fast Dijkstra\u0026rsquo;s algorithm (C++) Strongly connected components \u0026copy; Eulerian Path (C++)  Data structures  Suffix arrays (C++) Binary Indexed Tree Union-Find Set (C/C++) KD-tree (C++) Lazy Segment Tree (Java) Lowest Common Ancestor (C++)  Miscellaneous  Longest increasing subsequence (C++) Dates (C++) Regular expressions (Java) Prime numbers (C++) C++ input/output Knuth-Morris-Pratt (C++)  Stavropol SU : Extremely outdated, but worth to look at.  vimrc Java template Combinatorics Number Theory String Algorithms Min-cost max-flow Graph Theory Games Geometry Math Data Structures Miscellanious 13FFT     Special Thanks: My trainer Tarif Ezaz and my friend Mohammad Abdullah Matin Khan Zarzis to whom I learned to think.\nI also want to mention some of the other special names for their tremendous support. Nafis Ahmed, Mohammad Samiul Islam, Zobayer Hasan, Forhad Ahmed and Leonardo Boshell\nNOTE : I don\u0026rsquo;t claim all of the soutions to be mine. While I was solving the problems, I took help from different peoples and see other people\u0026rsquo;s code for many problems. In Fact most of the coder here is collected. But I never submit any code without my complete understanding. I suugest those who will be following the repo to do so. Pasting code to online judges won\u0026rsquo;t take you any further except frustration.\n","date":1548795600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548795600,"objectID":"01c23b42f68c0c2260cf4b7ed5765fe1","permalink":"https://sbmaruf.github.io/project/algorithms-code-library/","publishdate":"2019-01-30T00:00:00+03:00","relpermalink":"/project/algorithms-code-library/","section":"project","summary":"C++ implementation of various algorithms.","tags":["Algorithms"],"title":"Algorithms-Code-Library","type":"project"},{"authors":null,"categories":null,"content":" 2024 Received MIT \"Innovators Under 35\" Award.   2019 NTU Research Scholarship, Fully funded Ph.D. scholarship for 4 years.    2016 6th/100+ in Inter University Programming Contest, NSU Cybernauts National Programming Contest    2016 2nd/100 in Inter University Programming Contest, Daffodill International University ACM ICPC world finals warmup contest 2016    2014 Honorable Mention, Human Expedition on Mars Timeline 2018.    2014 Champion, IUT Computer Programming Contest.    2012 OIC Scholarship for undergraduate study, Islamic University of Technology.    ","date":1548363600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548363600,"objectID":"cf8de4cf088739dbf135a9034ab2931a","permalink":"https://sbmaruf.github.io/post/awards/","publishdate":"2019-01-25T00:00:00+03:00","relpermalink":"/post/awards/","section":"post","summary":"2024 Received MIT \"Innovators Under 35\" Award.   2019 NTU Research Scholarship, Fully funded Ph.D. scholarship for 4 years.    2016 6th/100+ in Inter University Programming Contest, NSU Cybernauts National Programming Contest    2016 2nd/100 in Inter University Programming Contest, Daffodill International University ACM ICPC world finals warmup contest 2016    2014 Honorable Mention, Human Expedition on Mars Timeline 2018.    2014 Champion, IUT Computer Programming Contest.","tags":null,"title":"HONORS \u0026 AWARDS","type":"post"},{"authors":null,"categories":null,"content":"This is a tool to translate an English sentence into Malay and vice versa. Developing a translation tool for low-resource languages like Malay has always been a challenge. The main challenge comes from the fact that machine translation systems typically rely on a huge amount of sentence-parallel data, and creating such datasets is an expensive process. In our work, we collected parallel datasets from various sources including News, OpenSubtitiles (OPUS), Ted talks, and Youtube video. Therefore, our corpus is quite generic and covers both texts and conversations.\nWe used various state of the art deep Neural Machine Translation (NMT) architecture for training our model. More specifically we use both seq2seq and transformer-net architecture for finding our best model. For pre-processing and post-processing datasets we used various tools of moses. To train our model we used OpenNMT-py framework which is very standard in the NMT community for it\u0026rsquo;s robust and modular implementation.\nCurrently the live demo can only be accessed from inside NTU network. \n","date":1548363600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548363600,"objectID":"a0346612fe8c4e999079b44dc91b23c6","permalink":"https://sbmaruf.github.io/project/mt-system/","publishdate":"2019-01-25T00:00:00+03:00","relpermalink":"/project/mt-system/","section":"project","summary":"This is a tool to translate an English sentence into Malay and vice versa.","tags":["Deep Learning"],"title":"Malay-English Neural Machine Translation System.","type":"project"},{"authors":null,"categories":null,"content":" Language models are trained with millions of dollars, involving very intricate engineering jobs, as well as thousands of hours of manpower and human stresses. However, day by day, the evaluation of language models is becoming so confusing and cumbersome that it becomes almost impossible to confidently say which language model is better at what. In this article, I have tried to sketch out some of my observations and findings, provide some recipes, and conclude with a call for a more rigorous approach to language model evaluation.\nTable of Content  What makes a good evaluation suite for leaderboard?  Evaluation Benchmarks  NLP Tasks Programming Language Tasks  What to Evaluate? Explainability of the evaluation outcome?  Performance indicators  Pretrained Language Model evaluation vs ChatModel evaluation The confusion on rank based evaluation and generative evaluation The irony of generative metrics? The True Generative Evaluation?  When evaluation should be automated and when human in a loop? Metric learning Evaluation bias, Can (\u0026amp; When) LLM-evaluation be a good indicator? Example of an ideal evaluation scenario? A call for more rigour evaluation  Total number of vocabulary covered? Total number of domain covered? Training token vs Evaluation token Use of a common prompting framework Share Inference Outputs Evaluation hyperparameters Few-shot vs Zero-shot evaluation Prioratizing manual evaluation (author vs independent evaluation) Avoid Anthromorphizing Whom to credit?  Evaluation is not solved  Multi-hop testbed Agentic evaluation   Prerequisites: I assume the readers have the following knowledge: - The currect tide of LLM evaluation crisis. - Interaction with one or more LLM in form of ChatModel (i.e., ChatGPT, Vicuna) and pretrained model (i.e., T5, GPT-J, GPT-3)\nWhat makes a good evaluation suite for leaderboard? NLP community has been obssessed by leaderboards. Before going to the discussions, lets list out some of the popular evaluation benchmarks and datasets with minimal statistics. (If the tables are too exaustive for you, please go ahead to the next section from the Table of Content)\nEvaluation Benchmarks NLP Tasks Here is a summary of a list of benchmarks NLP Tasks (in no particular order),\n   Benchmark - Task Type Dataset Split No. of Samples     GLUE      \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;Grammatical Acceptability, NLU CoLA dev 1043   \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;Sentiment, NLU SST-2 dev 872   \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;Paraphrases, NLU MRPC dev 408   \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;Paraphrases, NLU QQP dev 40430   \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;Score 0-5 , NLU STSB dev 1500   \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;Natural Language Inference MNLI dev 19647   \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;Natural Language Inference, Squad.V1 QNLI dev 5463   \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;Natural Language Inference RTE [1] 2 [2] [3] [4] dev 277   \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;Natural Language Inference WNLI dev 71   \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;Natural Language Inference AX test 1104   SuperGLUE      Binary QA BoolQ Dev 3270   \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;Natural Language Inference CB Dev 56   \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;Causal Reasoning COPA Dev 100   \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;Reading Comprehension (True/False) MultiRC Dev 4848   \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;Multiple-choice QA ReCoRD Dev 10000   \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;Natural Language Inference RTE [1] 2 [2] [3] [4] Dev 278   \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;Word-in-Context WiC Dev 638   \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;Reading Comprehension with Pronoun WSC Dev 104   \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;Natural Language Inference AX-b Test 1104   \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;Gender Bias AX-g Test 356   BIG-bench (Crowdsourced Massive Multitask) Task List Test 214 task (not number of sample)   Big-Bench Hard 23 hand picked tasks from Big-Bench Test 6511   Humanities, Social Sciences, STEM etc. MMLU Test 14042   Inverse Scaling Challenge 11 tasks CoT/Direct 1808   Ethics Benchmark 5 subset of task, Justice, Virtue, Deontology Utilitarianism, Commonsense Test/Hard Test 19968\u0026frasl;18604   Open Domain QA      \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;Trivia QDQA TriviaQA (rc) Dev 17944   \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;Wiki QDQA NQ-Open Dev 3610   \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;Single Named Entity based ODQA WebQuestions Test 2032   \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;Subset of Wiki ODQA EfficientQA Dev 1800   Reading Comprehension       Race-Middle Test 1436    Race-High Test 3498    [SQuAD-V2]( ) Dev 11873   Common Sense Reasoning       PIQA Dev 1838    SIQA Dev 1954    HellaSwag Dev 10042    WinoGrande Dev 1267    ARC-Easy Test 2376    ARC-Challenge Test 1172    OBQA Test 500   Mathematical Reasoning       MATH Test 5000    GSM-8k Test 1319    MGSM Test, 11 lang 2750   Natural Language Inference       ANLI R1, R2, R3 Test 3200    MNLI dev 19647    XNLI Test, 15 lang 75150   Text Summarization       CNN/DM [1][2] Test 11490    XSUM Test 11334    SAMSum Test 819    DialogSum Test 500   Bias and Misinformation       Gender Bias Test 356    WinoBias Test 1580    TruthfulQA Test 817   Sequence Labeling      Named Entity Recognition CoNLL 2003 Shared Task Test on 2 lang 6458   Named Entity Recognition CoNLL 2002 Shared Task Test on 2 lang 6593   Named Entity Recognition Bari et al 2020 Test on 2 lang 3766   Parts of Speech tagging Universal Dependencies 2.7 Test on 2 lang 5427   Named Entity Recognition WikiANN 473000    XGLUE      \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Named Entity Recognition CoNLL 2002 2003 Shared Task Test 13186   \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Parts of Speech tagging UD 2.5 test    XTREME      LinCE      GLUECoS      GENIE      Long Arena      GEM      DialoGLUE       Programming Language Tasks And here is a list of programming language code evaluation benchmark taken from MAM Khan et. al.\n   Dataset Train Test La Task Type Evaluation Level Genre     Django 16,000 1,805 1 Program Synthesis Lexical Local N/A   WikiSQL 56,355 15,878 1 SQL Queries Lexical Modular SQL   Antonio Valerio Miceli Barone et al 109,108 2,000 1 Synthesis, Summarization Lexical Local Github   CoNaLa 2,379 500 2 Program Synthesis Lexical Local Stackoverflow: QA   CONCODE 100,000 2,000 1 Program Synthesis Lexical Modular Github   Android 26,600 3,546 1 Program Synthesis Lexical Local Map oriented, GitHub   CodeSearchNet 6,452,446 99 6 Plain Text, Retrieval NDCG Modular Github   JuICe 1,518,049 1,981 1 Notebook Cell Gen. Lexical Local Prog. assignment   TransCoder 721MB 1,410 3 Program Translation Lexical Modular Github   HumanEval - 164 1 Program Synthesis Execution Modular Interview Question   HumanEval-X - 820 9 Synthesis \u0026amp; Translation Execution Modular Interview Question   MBPP - 974 1 Program Synthesis Execution Modular Interview Question   CodeXGLUE 2,840,000 759,000 9 10 Tasks Lexical Local N/A   AVATAR 5,937 1,693 2 Program Translation Lexical Global Problem Solving   TFix 84,846 10,504 1 Program Repair Lexical Local Github   CCSD 84,316 6,533 1 Program Summarization Lexical Modular Linux Kernel   TL-CodeSum 55,766 6,971 1 Program Summarization Lexical Modular Github   CodeNet 8,906,769 2,783,365 55 Classification, similarity Lexical Global Problem Solving   TransCoder-ST 333,542 103,488 3 Program Translation Execution Modular Github   DSP - 1,119 1 Notebook Cell Gen. Execution Local Math and Data Science   MTPB - 115 1 Multi-turn Code Gen. Execution Local Problem Solving   Exe-DS 119,266 534 1 Notebook Cell Gen. Execution Local Data Science   DS-1000 - 1,000 1 Notebook Cell Gen. Execution Local Data Science   MoCoNaLa - 896 1 Program Synthesis Lexical Local StackOverflow   ARCADE - 1,082 1 Notebook Cell Gen. Lexical Local Data Science   ODEX - 945 1 Program Synthesis Execution Local StackOverflow   MBXP - 13,877 10 Program Synthesis Execution Modular Interview Question   XLCoST 496,333 45,394 7 10 Task Lexical Local, Global GitHub   DeepFix 37,000 7,000 1 Program Repair Ececution Global Compile Error, Students   Defects4J - 835 1 Program Repair Execution Local, Global N/A   APPS 5,000 5,000 1 Program Synthesis Execution Global Interview Question   CodeContests 4,432,447 32,181 3 Program Synthesis Execution Global Problem Solving   CoderEval - 460 2 Program Synthesis Execution Modular, Global GitHub   xCodeEval 19,915,150 159,464 17 7 Tasks Execution Global Problem Solving    What to Evaluate? It\u0026rsquo;s an easy question, but hard to answer. The way most of the research group and Ph.D. student evaluate LLM is, How can I create more evaluation tables with minimal effort. This is because immense publication pressure as well as the Fear of missing out. Let\u0026rsquo;s see an example. Now in the western culture, \u0026ldquo;pronoun\u0026rdquo; or how someone identify himself is a big social movement that\u0026rsquo;s being also included in the law. But none of the language model evaluation now a days evaluate Named Entity Recognition (NER) or Parts of Speech tagging (POS) since it\u0026rsquo;s evaluation is a bit complicated and sometimes requires domain extertise on interpreting results. However it is inherintly important to evaluate LLMs understanding of NER or POS tagging. Also may be NER or POS tagging is not flashy enough to tweet (!!!).\nSo as a researcher, it is important to stating two important aspect,\n What I\u0026rsquo;m evaluating. What\u0026rsquo;s missing in the evaluation.  Decades of benchmark creation effort certainly has a value embedded rather than finding cherry picked flashy LLM fails. The above table summarizes a good NLP evaluation benchmarks. However it fails miserably on many different aspects like,\n Evaluation on many Specific Social Science aspects. Evaluation on Political biases. Evaluation on Harmfullness and Helpfulness.  There\u0026rsquo;s been amazing continious effort of making LLM less Harmfull and effectively helpful at Anthropic. I would recommend few article on this if you have more interest on these topics.\nIt\u0026rsquo;s been written in Open AI System card that their LLMs are evaluted on 50 Red teamming team(s) or individial experts on the certain fields. It\u0026rsquo;s a shame that it\u0026rsquo;s not even mentioned that what are the red teaming topics that they considered. This trend of not sharing the details due to citing industry competitiveness will surely continue until opensource cathces up on Red Teaming. I see a lot of Opensource community is driving LLM research like EleutherAI, BigScience, Aya. It would be great to have them picking up the Red Teaming effort. I cannot see a world where red teaming is unsuccessfull within opensource community.\nThat being said, I\u0026rsquo;ll conclude this section by inviting researchers to always summarize their evaluation outcome and adding limitations. When a new LLM comes in wilderness, in the paper (or now a days so called technical report prepared for VC funding), there should be a summary of evaluation outcome, Why a set of evaluation benchmarks/protocols are selected to evaluate what downstream capability?.\narrangement citation collaspe\nwhats wrong with eval datasets importance of time training mmetadata of time twitter agent\n","date":1548363600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548363600,"objectID":"3a7d25f0f5e8538d45e8015400e38c7d","permalink":"https://sbmaruf.github.io/post/eval/","publishdate":"2019-01-25T00:00:00+03:00","relpermalink":"/post/eval/","section":"post","summary":"Language models are trained with millions of dollars, involving very intricate engineering jobs, as well as thousands of hours of manpower and human stresses. However, day by day, the evaluation of language models is becoming so confusing and cumbersome that it becomes almost impossible to confidently say which language model is better at what. In this article, I have tried to sketch out some of my observations and findings, provide some recipes, and conclude with a call for a more rigorous approach to language model evaluation.","tags":null,"title":"What's wrong with LLM Evaluation?: Evaluation is Hard (\u0026 also Costly).","type":"post"},{"authors":["M Saiful Bari"],"categories":null,"content":"","date":1537477200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537477200,"objectID":"4903b68db721accb86404608fd07e3b8","permalink":"https://sbmaruf.github.io/publication/data-analytics-book/","publishdate":"2018-09-21T00:00:00+03:00","relpermalink":"/publication/data-analytics-book/","section":"publication","summary":"This book chapter goes through various aspects of regression and maths behind them.","tags":["regression","data-analytics","book"],"title":"Data Analytics: Concepts, Techniques and Applications","type":"publication"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536440400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536440400,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"https://sbmaruf.github.io/tutorial/example/","publishdate":"2018-09-09T00:00:00+03:00","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"},{"authors":[],"categories":null,"content":"Special thanks to the author Emma Strubell for replying some of the query by mail.\n Click on the **Slides** button above to view the built-in slides feature.   -- ","date":1524655800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524655800,"objectID":"fc1a304f7347839c0880710f727ffb81","permalink":"https://sbmaruf.github.io/talk/idcnn/","publishdate":"2018-04-25T00:00:00+03:00","relpermalink":"/talk/idcnn/","section":"talk","summary":"This talk summarizes the paper [`Fast and Accurate Entity Recognition with Iterated Dilated Convolutions`](https://arxiv.org/abs/1702.02098).","tags":["Deep-Learning"],"title":"Iterated Dilated Convolutions for NLP - NER as an example","type":"talk"},{"authors":[],"categories":null,"content":" Click on the **Slides** button above to view the built-in slides feature.   -- ","date":1521631800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1521631800,"objectID":"2f728de662943cb2c7f01ad9606f77a0","permalink":"https://sbmaruf.github.io/talk/attention/","publishdate":"2018-03-21T00:00:00+03:00","relpermalink":"/talk/attention/","section":"talk","summary":"This talk summarizes the paper [`Effective Approaches to Attention-based Neural Machine Translation`](https://arxiv.org/abs/1508.04025).","tags":["Deep-Learning"],"title":"Attention Mechanism","type":"talk"},{"authors":[],"categories":null,"content":" Click on the **Slides** button above to view the built-in slides feature.   -- ","date":1521023400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1521023400,"objectID":"1eab63e31962551d6962e6ba8f7f9b58","permalink":"https://sbmaruf.github.io/talk/encode-decode-architecture/","publishdate":"2018-03-14T00:00:00+03:00","relpermalink":"/talk/encode-decode-architecture/","section":"talk","summary":"This talk summarizes the paper [`Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation`](https://arxiv.org/abs/1406.1078).","tags":["Deep-Learning"],"title":"Encode Decode Architecture","type":"talk"},{"authors":[],"categories":null,"content":" Click on the **Slides** button above to view the built-in slides feature.   -- ","date":1520431200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520431200,"objectID":"098067c2d70f6fdfaf7b7e6af86ee4b1","permalink":"https://sbmaruf.github.io/talk/structure-of-rnn-cells/","publishdate":"2018-03-07T00:00:00+03:00","relpermalink":"/talk/structure-of-rnn-cells/","section":"talk","summary":"A talk on the structures of RNN cells.","tags":["deep-learning","architecture"],"title":"Structure of RNN Cells","type":"talk"},{"authors":["M Saiful Bari"],"categories":null,"content":" Academic makes it easy to create a beautiful website for free using Markdown. Customize anything on your site with widgets, themes, and language packs.\nFollow our easy step by step guide to learn how to build your own free website with Academic. Check out the personal demo or the business demo of what you\u0026rsquo;ll get in less than 10 minutes.\n View the documentation Ask a question Request a feature or report a bug Updating? View the Update Guide and Release Notes Support development of Academic:  Donate a coffee Become a backer on Patreon Decorate your laptop or journal with an Academic sticker Wear the T-shirt   \nKey features:\n Easily manage various content including homepage, blog posts, publications, talks, and projects Extensible via color themes and widgets/plugins Write in Markdown for easy formatting and code highlighting, with LaTeX for mathematical expressions Social/academic network linking, Google Analytics, and Disqus comments Responsive and mobile friendly Simple and refreshing one page design Multilingual and easy to customize  Color Themes Academic is available in different color themes and font themes.\n         Ecosystem  Academic Admin: An admin tool to import publications from BibTeX or import assets for an offline site Academic Scripts: Scripts to help migrate content to new versions of Academic  Install You can choose from one of the following four methods to install:\n one-click install using your web browser (recommended) install on your computer using Git with the Command Prompt/Terminal app install on your computer by downloading the ZIP files install on your computer with RStudio  Quick install using your web browser  Install Academic with Netlify  Netlify will provide you with a customizable URL to access your new site  On GitHub, go to your newly created academic-kickstart repository and edit config.toml to personalize your site. Shortly after saving the file, your site will automatically update Read the Quick Start Guide to learn how to add Markdown content. For inspiration, refer to the Markdown content which powers the Demo  Install with Git Prerequisites:\n Download and install Git Download and install Hugo   Fork the Academic Kickstart repository and clone your fork with Git:\ngit clone https://github.com/sourcethemes/academic-kickstart.git My_Website  Note that if you forked Academic Kickstart, the above command should be edited to clone your fork, i.e. replace sourcethemes with your GitHub username.\n Initialize the theme:\ncd My_Website git submodule update --init --recursive   Install with ZIP  Download and extract Academic Kickstart Download and extract the Academic theme to the themes/academic/ folder from the above step  Install with RStudio View the guide to installing Academic with RStudio\nQuick start  If you installed on your computer, view your new website by running the following command:\nhugo server  Now visit localhost:1313 and your new Academic powered website will appear. Otherwise, if using Netlify, they will provide you with your URL.\n Read the Quick Start Guide to learn how to add Markdown content, customize your site, and deploy it. For inspiration, refer to the Markdown content which powers the Demo\n Build your site by running the hugo command. Then host it for free using Github Pages or Netlify (refer to the first installation method). Alternatively, copy the generated public/ directory (by FTP, Rsync, etc.) to your production web server (such as a university\u0026rsquo;s hosting service).\n  Updating Feel free to star the project on Github to help keep track of updates and check out the release notes prior to updating your site.\nBefore updating the framework, it is recommended to make a backup of your entire website directory (or at least your themes/academic directory) and record your current version number.\nBy default, Academic is installed as a Git submodule which can be updated by running the following command:\ngit submodule update --remote --merge  Check out the update guide for full instructions and alternative methods.\nFeedback \u0026amp; Contributing Please use the issue tracker to let me know about any bugs or feature requests, or alternatively make a pull request.\nFor support, head over to the Hugo discussion forum.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1461099600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515790800,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://sbmaruf.github.io/post/getting-started/","publishdate":"2016-04-20T00:00:00+03:00","relpermalink":"/post/getting-started/","section":"post","summary":"Create a beautifully simple website or blog in under 10 minutes.","tags":["Academic"],"title":"Academic: the website designer for Hugo","type":"post"},{"authors":null,"categories":null,"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"https://sbmaruf.github.io/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$","tags":null,"title":"Slides","type":"slides"}]